AmazonEKSComputePolicy:
=======================
ENI management
Networking operations
Cluster-related actions:
--------------------------
Pull images from Amazon ECR.
Log to CloudWatch (if configured).


AmazonEKSBlockStoragePolicy:
============================
for ebs volumes



* kubectl is a tool provided by eks cluster to connect the cluster through cli
* install kubectl
* kubectl version : will get an error like not to connect to server
* kubectl communicates to eks cluster ;;; between kubectl and eks cluster there is an yaml file configuration i.e.kubeconfig
* A kubeconfig is a YAML file that stores cluster access details. It can be provided manually, or generated via the CLI using:
aws eks update-kubeconfig --name <cluster-name>
*



There are two ways to manage Kubernetes resources: imperative and declarative.
Kubernetes resources can be managed in two ways:
Imperative::: kubectl create deployment nginx --image=nginx
Declarative::: kubectl apply -f deployment.yaml

Kubernetes resources:
=====================
Pod
Deployment
ReplicaSet
etc....

Imperative::: kubectl create deployment nginx --image=nginx
kubectl get pods
kubectl get pod nginx -o yaml
*** disadv: manually each and every step we need to write and don't easy to track where the mistakes are there.

List of api resources:
======================
kubectl api-resources

List of versions:
=================
kubectl api-versions


ex1:
---
apiVersion: v1
kind: Pod
metadata:
 name: nginx
containers:
 - name: nginx
   image: nginx


Pod is not immediately created on a node.
First, the API server knows about it, then the scheduler assigns it to a node, and finally the Kubelet on that node starts the Pod.

to enter the shell/bash of a container running in a Pod:
-------------------------------------------------------
"kubectl exec -it demo -- bash is the command used"


multi-containers:
-----------------
apiVersion: v1
kind: Pod
metadata:
 name: multi-containers
containers:
  - name: nginx
    image: nginx
  - name: tomcat
    image: tomcat


** to know Pod ip address : kubectl get pod <pod-name> -o wide
** kubectl describe pod <pod-name>: to know node ip address

IP = Pod IP
Node = Node it’s running on

To know the status of POD Ip: kubectl get pod <pod-name> -o yaml


Shared disk in Kubernetes:
==========================
* A shared disk (volume) is a storage resource that can be mounted by multiple containers.
* It doesn’t automatically belong to multiple containers — you have to mount it in each container you want to access it.


In Docker (single-host)
=========================
* By default, Docker cannot share a local volume across containers on different hosts.
* You can share a volume between containers on the same host by mounting the same volume into multiple containers:
check containerd status:
=========================
* sudo systemctl status containerd
Check containerd version:

containerd --version


Example output:

containerd containerd.io 1.6.18


Volumes:
========
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: sample

  - image: nginx-content
    name: content
    volumeMounts:
    - mountPath: /opt
      name: sample
  volumes:
  - name: sample
    emptyDir:
      sizeLimit: {}

 here /opt is a mount path with an empty directory with the reference name sample  in same pod
 ===========================================================================================================
 Volume sharing depends on the volume, not the mountPath name
 ============================================================
 Volume name → defines the actual storage. ✅
  mountPath → is just where the volume appears inside the container.
  Volume (sample) = storage/disk
  MountPath (/data) = directory inside container pointing to that disk


=====================================================================================================================================
kubectl exec -it <pod-name> -c nginx -- ls -l /usr/share/nginx/html , check file permissions
rw------- → only root can read → nginx (running as nginx user) cannot serve → 403


kubectl apply -f "pod.yaml"
kubectl get pods
kubectl exec -it pod-name -- bash
kubectl exec -it demo2 -c nginx -- bash

kubectl describe pod "podname"
kubectl logs "podname"
 kubectl logs demo2 -c nginx
kubectl logs podname -c "container-name"
kubectl get pod podname -o yaml
kubectl get pod podname -o wide
** when image pulls from docker hub , if an image create a container and exit immediately : crashLoopBackOff
** then add tail -f /dev/null ; to run container infinite times
* curl localhost
network sharing:
================
Pod: web-pod (IP: 10.0.0.5)
├─ Container: nginx (Port 80)
├─ Container: redhat (can reach nginx via localhost:80)
└─ Shared:
   ├─ Network namespace
   ├─ Loopback (localhost)
   └─ Pod IP (10.0.0.5)

Network sharing: use ports → containers in same Pod can connect via localhost:<port>
Filesystem sharing: use a shared volume → files are visible to all containers mounting it

Images vs services
====================
Image = blueprint of a container; it contains files, libraries, configuration, etc.
Container = a running instance of the image.
Service inside the container = a process that listens on a port (e.g., nginx web server, tomcat app server).
Only services expose data out via network. The raw files inside the image are not automatically exposed.


Old way (pre-1.24):
Kubelet --> DockerShim --> Docker Engine --> Runs container

New way (1.24+):
Kubelet --> containerd (or CRI-O) --> Runs container



ephemeral containers:
---------------------
kubectl debug -it <pod-name> --image=busybox --target=<target-container-name> -- /bin/sh
kubectl debug -it my-pod --image=ubuntu --target=app
--image: image with debugging tools
--target: container whose namespaces (PID, network, filesystem) you want to share
-it: interactive terminal

* kubectl debug -it --attach=true -c debug --image=redhat/ubi9 test-nginx

****************************************************************************************
1. create a dockerfile under ephemeral-container folder
2. make a file with name "makefile" under this
ephemeral-practice:
docker build -t pavanidevops78/kubernetes ephemeral-container
docker push pavanidevops78/kubernetes
3. docker run -rm -it --entrypoint bash docker.io/pavanidevops78/kubernetes:ec, this is overriding concept if there is a sh command we need to replace bash
if there is no command will get an error there is no bash/sh
4. if a container created then to enter inside shell/bash container otherwise no

two ways to debug:
==================
* through pod
* kubectl debug -it --attach=true -c debug --image=docker.io/pavanidevops/kubernetes:ec test-nginx
* kubectl debug -it --attach=true -c debug --image=docker.io/pavanidevops/kubernetes:ec

firstway:
---------
apiVersion: v1
kind: Pod
metadata:
 name: ephemeral-container
spec:
 containers:
  - name: ephemeral-image
    image: pavanidevops78/kubernetes:ec

---
// debug with anoher pod

apiVersion: v1
kind: Pod
metadata:
 name: sample
spec:
 containers:
  - name: nginx-content
    image: pavanidevops78/kubernetes:v1

// kubectl debug, this container is ephemeral
// this image doesn't contain sh/bash so container won't create
// container will create but not to start
// so to troubleshoot this container

kubectl exec -it ephemeral-image -- bash or -- sh
kubectl exec -it ephemeral-image -- curl localhost:8080

troubleshoot:
-------------
* create a container inside same pod: nginx
two containers are created under same pod
get container ip address: kubectl get pod -o wide
so network namespaces are same so we can able to get data inside a shell/bash container using : curl:http://....:8080
if both are having service then only network sharing is possible

Labels:
=======
** In EKS / Kubernetes, labels are the standard way to identify, filter, and group applications.
** Labels are key-value pairs attached to Kubernetes resources (pods, deployments, services, etc.).
** They are metadata used for identification, selection, and grouping.

a) List pods with a specific label:
===================================
kubectl get pods -l app=my-app

-l → label selector
Returns only pods with app=my-app

* kubectl get pods --help
by default show_labels are false, so changed to true then only we can visible


Flow of control plane:
======================
kubernetes identity:
=====================
User
Groups
ServiceAccount (special case for workloads)
1. create an eks cluster
2. create a node group for ec2 instances
3. install kubectl
4. kubectl version
5. kubectl read kubeconfig to communicate to api-server: aws eks update-kubeconfig -- name dev
6. kubectl is a tool at client side
7. kubectl apply -f 01-pod.yaml, here kubectl communicates to api-server , here first api-server authenticate iam identity. here we have to follow a few steps:
* where cluster installed in ec2 resources [workstation_role] required some policies what cluster should allow[crud operations]
* kubectl is a client side tool to communicate to api-server, first workstation server role has cluster policies or not.
* here access entry will come
* add workstation iam role and add polices
* now kubectl has a permission to communicate to api-server[authenticate: iam identity+policies , authorization: map iam identity to kubernetes groups, bearer token generate by using iam identity +policies]
* to get token: aws eks get-token --cluster-name dev
* api-server write pod spec into etcd
* scheduler assigns a node for pod
* kubelet create a container for that pod
8. kubectl get pods
api-server get pod objects from etcd

how container create:
=====================
Pod spec always stored in etcd first
Scheduler updates the Pod object with node assignment in etcd
Kubelet reads the Pod spec via API server — it never reads etcd directly
Container creation happens after node assignment

Replicaset Controller:
=======================
kubectl delete pod
     │
     ▼
API server updates Pod deletionTimestamp → etcd
     │
     ▼
ReplicaSet controller notices Pod missing → create new Pod object → API server → etcd
     │
     ▼
Kubelet on new node reads Pod spec → starts container → updates Pod status → API server → etcd


kubectl get pods --watch

* kubectl get pods  --show-labels=true
* kubectl get pods  -l app=nginx , here -l is a selector
* kubectl scale rs rs01 replicas=3, here scale means to increase number of replicas
// here rs is a shortcut for replicas ,rs01 is a replica name , replicas=3 to increase number of replicas



environment variables:
-----------------------
1.List the running Pods:
kubectl get pods -l purpose=demonstrate-envars

2.List the Pod's container environment variables:
kubectl exec envar-demo -- printenv

3.


kubernetes resources:
======================
secret:
-------
* Create the Secret: kubectl apply -f "*.yaml"
* View information about the Secret: kubectl get secret test-secret
* View more detailed information about the Secret: kubectl describe secret test-secret


** services in kubernetes:
===========================
Container (my-app):
  listens on 8080 (application port / containerPort)

Node (EC2 instance):
  exposes 3000 (hostPort)

Traffic flow:
External client → NodeIP:3000 → Container:8080 → Application

Application port is inside the container; hostPort is on the node and maps to the container port. They can be the same, but don’t have to be — hostPort is optional and limited to one Pod per node.


-------------------------------------------------------------------------------------------------------------------------------
volumes:
=========
Container A (writer) → writes to /shared-data (volume mount)
Volume (emptyDir, PVC, etc.) → stores the data
Container B (reader) → reads from /other-path (mounted same volume)


Services in eks:
================
* earlier pod to pod communication through ip address is available, but every time ip address should not be same because when we restart a pod at that time ip address also changed.
* to avoid this problem we have to go "Service" in kubernetes.
* Service creates a constant ip address or dns name based on this we have to communicate to the pod

important points:
=================
* Pod ip's are not stable: don't use them directly to the communication
* Service provides a stable IP and DNS name: use this to communicate reliably
* This works for Pod to Pod communication inside the clsuter

* host Port : node port [ec2 instance]
* container/application port : target port or containerPort
* service port: user provide service port and created inside an eks cluster[ClusterIP,NodePort,LoadBalancer]
* targetPort: Service-Pod

Client → ServicePort (user-defined) → targetPort (Pod containerPort) → Application

Inside the cluster: cluster uses ServicePort to reach Pods
Outside the cluster: If NodePort or LoadBalancer , traffic goes to NodeIP: NodePort ----> Service -----> Pod


Flow how service type to application connects:
==============================================
* LoadBalancer receives client request
* LoadBalancer connect to service port (user provided port)
* Service Port connects to the container Port i.e targetPort [pod containers port]
* container port is an application port where the application process will happen
* finally pod send back traffic  to the service

Client → LoadBalancer (public IP)
        ↓
Service (ServicePort, e.g., 8000)
        ↓
Pod (targetPort / containerPort, e.g., 80)
        ↓
Application (nginx, Node.js, etc.) processes request
        ↑
Pod → Service → LoadBalancer → Client (response)


2. for single pod multiple ports:
=================================
Databases often use multiple ports, and that’s why we expose multiple ports in a single Pod

for example:
make a dockerfile for node.js
1.take two ports to listen server node.js
2.take one pod
3.take one service
4.connect both pod and service with labels
5.open ports in both pod and service

3. when two containers expose with same port in a node then we will get a problem:
docker run -d -p 8000:80 nginx [to run the same application port two times we will get a problem]


kube-proxy:
===========
* kube-proxy watches service and endpoints not pods
* using that information, kube-proxy enables internal L4 load balancing rules

EndPoint:
=========
it contains   PodIP + targetPort(container IP)

What kube-proxy actually watches?
===================================
kube-proxy watches these API objects:

* Service
* EndpointSlice (preferred) or Endpoints
* EndpointSlice contains:  PodIP + targetPort

Pods → EndpointSlice → kube-proxy rules

internal L4 Load balancing rules:
---------------------------------
kube-proxy:

1.does not forward packets itself
2.installs iptables or IPVS rules in the Linux kernel
3.kernel chooses one Pod endpoint per connection
This is Layer 4 (TCP/UDP) load balancing.

Flow summary:
==============
Pods (labels)
   ↓
EndpointSlice
   ↓
kube-proxy (watching)
   ↓
iptables / IPVS rules
   ↓
Kernel load-balances


Flow how traffic routes from cluster service to the POD:
========================================================
* when a Kubernetes Service forwards traffic to a Pod, it involves NAT
* First hit client ip address, translate service ip and then pod ip address
* when you hit a node port[means hitting a outside server]
* kube-proxy is a networking layer between kubernetes.
* service forward the traffic to the pod
* click on Network interface and check which instance is connected to this interface
* In network interfaces there are multiple ip address.
* listen address: 0.0.0.0 , in network interfaces have multiple ip address to connect
* any ip address associated to process , then process will send traffic to ip address.
*
how Loadbalancer creates?

User
  |
  v
Kubernetes API → Service type: LoadBalancer
  |
  v
Cloud Controller Manager
  |
  v
AWS API → Provision ELB / register nodes as targets
  |
  v
Load Balancer (external IP/DNS)
  |
  v
Nodes (NodePort)
  |
  v
kube-proxy → Pod (on same or different node)


















